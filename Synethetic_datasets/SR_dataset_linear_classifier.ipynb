{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306eca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import time\n",
    "from itertools import product\n",
    "from json import dumps\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "from torch_geometric.nn import DataParallel\n",
    "from torch_geometric.seed import seed_everything\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_metric_learning.losses import NTXentLoss, VICRegLoss\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from datasets.SRDataset import SRDataset\n",
    "\n",
    "import train_utils\n",
    "from data_utils import extract_edge_attributes\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from layers.input_encoder import LinearEncoder, LinearEdgeEncoder\n",
    "from layers.layer_utils import make_gnn_layer\n",
    "from models.GraphClassification import GraphClassification\n",
    "from models.model_utils import make_GNN\n",
    "\n",
    "import GCL.losses as L\n",
    "import GCL.augmentors as A\n",
    "from GCL.eval import get_split, SVMEvaluator, LREvaluator\n",
    "from GCL.models import DualBranchContrast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a4e90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--feature_augmentation'], dest='feature_augmentation', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, required=False, help='If true, feature augmentation.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(f'arguments for training and testing')\n",
    "parser.add_argument('--save_dir', type=str, default='./save', help='Base directory for saving information.')\n",
    "parser.add_argument('--seed', type=int, default=2, help='Random seed for reproducibility.')\n",
    "parser.add_argument('--dataset_name', type=str, default=\"sr25\", help='Name of dataset')\n",
    "parser.add_argument('--drop_prob', type=float, default=0.6,\n",
    "                    help='Probability of zeroing an activation in dropout layers.') \n",
    "parser.add_argument('--batch_size', type=int, default=32, help='Batch size per GPU. Scales automatically when \\\n",
    "                        multiple GPUs are available.')\n",
    "parser.add_argument(\"--parallel\", action=\"store_true\",\n",
    "                    help=\"If true, use DataParallel for multi-gpu training\")\n",
    "parser.add_argument('--num_workers', type=int, default=0, help='Number of worker.')\n",
    "parser.add_argument('--load_path', type=str, default=None, help='Path to load as a model checkpoint.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Learning rate.')\n",
    "parser.add_argument('--l2_wd', type=float, default=5e-6, help='L2 weight decay.')\n",
    "parser.add_argument('--num_epochs', type=int, default=500, help='Number of epochs.')\n",
    "parser.add_argument(\"--hidden_size\", type=int, default=200, help=\"Hidden size of the model\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"KHopGNNConv\",\n",
    "                    choices=(\"KHopGNNConv\"), help=\"Base GNN model\")\n",
    "parser.add_argument(\"--K\", type=int, default=3, help=\"Number of hop to consider\")\n",
    "parser.add_argument(\"--num_layer\", type=int, default=3, help=\"Number of layer for feature encoder\")\n",
    "parser.add_argument(\"--JK\", type=str, default=\"sum\", choices=(\"sum\", \"max\", \"mean\", \"attention\", \"last\", \"concat\"),\n",
    "                    help=\"Jumping knowledge method\")\n",
    "parser.add_argument(\"--residual\", default=True, action=\"store_true\", help=\"If true, use residual connection between each layer\")\n",
    "parser.add_argument(\"--virtual_node\", action=\"store_true\", default=False, \n",
    "                    help=\"If true, add virtual node information in each layer\")\n",
    "parser.add_argument('--split', type=int, default=10, help='Number of fold in cross validation')\n",
    "parser.add_argument(\"--train_eps\", action=\"store_true\", help=\"If true, the epsilon in GIN model is trainable\")\n",
    "parser.add_argument(\"--combine\", type=str, default=\"geometric\", choices=(\"attention\", \"geometric\"),\n",
    "                    help=\"Combine method in k-hop aggregation\")\n",
    "parser.add_argument(\"--pooling_method\", type=str, default=\"sum\", choices=(\"mean\", \"sum\", \"attention\"),\n",
    "                    help=\"Pooling method in graph classification\")\n",
    "parser.add_argument('--norm_type', type=str, default=\"Batch\",\n",
    "                    choices=(\"Batch\", \"Layer\", \"Instance\", \"GraphSize\", \"Pair\"),\n",
    "                    help=\"Normalization method in model\")\n",
    "parser.add_argument('--aggr', type=str, default=\"add\",\n",
    "                    help='Aggregation method in GNN layer, only works in GraphSAGE')\n",
    "parser.add_argument(\"--patience\", type=int, default=20, help=\"Patient epochs to wait before early stopping.\")\n",
    "parser.add_argument('--factor', type=float, default=0.5, help='Factor for reducing learning rate scheduler')\n",
    "parser.add_argument('--reprocess', action=\"store_true\", help='If true, reprocess the dataset')\n",
    "parser.add_argument('--search', action=\"store_true\", help='If true, search hyper-parameters')\n",
    "parser.add_argument(\"--pos_enc_dim\", type=int, default=6, help=\"Initial positional dim.\")\n",
    "parser.add_argument(\"--pos_attr\", type=bool, default=False, help=\"Positional attributes.\")\n",
    "parser.add_argument(\"--feature_augmentation\", type=bool, default=False, help=\"If true, feature augmentation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7847c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ad4c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.name = args.model_name + \"_\" + str(args.K) + \"_\" + str(args.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038d0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging and devices\n",
    "args.save_dir = train_utils.get_save_dir(args.save_dir, args.name, type=args.dataset_name)\n",
    "log = train_utils.get_logger(args.save_dir, args.name)\n",
    "device, args.gpu_ids = train_utils.get_available_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e72c0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08.06.24 16:40:57] Using single-gpu training\n"
     ]
    }
   ],
   "source": [
    "if len(args.gpu_ids) > 1 and args.parallel:\n",
    "    log.info(f'Using multi-gpu training')\n",
    "    args.parallel = True\n",
    "    loader = DataListLoader\n",
    "    args.batch_size *= max(1, len(args.gpu_ids))\n",
    "else:\n",
    "    log.info(f'Using single-gpu training')\n",
    "    args.parallel = False\n",
    "    loader = DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f993f3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08.06.24 16:40:57] Using random seed 2...\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "seed = args.seed\n",
    "log.info(f'Using random seed {seed}...')\n",
    "seed_everything(seed)\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eea09a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_feature_transform(g):\n",
    "    return extract_edge_attributes(g, args.pos_enc_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1284626",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f48347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim):\n",
    "        super(LogReg, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = self.fc(x)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df2134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr25_train(loader, model, optimizer, device, parallel=False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        if parallel:\n",
    "            num_graphs = len(data)\n",
    "            y = torch.cat([d.y for d in data]).to(device)\n",
    "        else:\n",
    "            num_graphs = data.num_graphs\n",
    "            data = data.to(device)\n",
    "            y = data.y\n",
    "        out = model(data.graph_embeds).squeeze()\n",
    "        loss = F.cross_entropy(out, y.float())\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        optimizer.step()\n",
    "    return total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ebdd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sr25_test(loader, model, device, parallel=False):\n",
    "    model.train()  # eliminate the effect of BN\n",
    "    y_preds, y_trues = [], []\n",
    "    for data in loader:\n",
    "        if parallel:\n",
    "            y = torch.cat([d.y for d in data]).to(device)\n",
    "        else:\n",
    "            data = data.to(device)\n",
    "            y = data.y\n",
    "        y_preds.append(torch.argmax(model(data.graph_embeds), dim=-1))\n",
    "        y_trues.append(y)\n",
    "    y_preds = torch.cat(y_preds, -1)\n",
    "    y_trues = torch.cat(y_trues, -1)\n",
    "    return (y_preds == y_trues).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a24fd20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    layer = make_gnn_layer(args)\n",
    "    init_emb = LinearEncoder(args.input_size, args.hidden_size, pos_attr=args.pos_attr)\n",
    "    init_edge_attr_emb = LinearEdgeEncoder(args.edge_attr_size, args.hidden_size, edge_attr=True)\n",
    "    init_edge_attr_v2_emb = LinearEdgeEncoder(args.edge_attr_v2_size, args.hidden_size, edge_attr=False)\n",
    "    \n",
    "    GNNModel = make_GNN(args)\n",
    "    \n",
    "    gnn = GNNModel(\n",
    "        num_layer=args.num_layer,\n",
    "        gnn_layer=layer,\n",
    "        JK=args.JK,\n",
    "        norm_type=args.norm_type,\n",
    "        init_emb=init_emb,\n",
    "        init_edge_attr_emb=init_edge_attr_emb,\n",
    "        init_edge_attr_v2_emb=init_edge_attr_v2_emb,\n",
    "        residual=args.residual,\n",
    "        virtual_node=args.virtual_node,\n",
    "        drop_prob=args.drop_prob)\n",
    "\n",
    "    model = GraphClassification(embedding_model=gnn,\n",
    "                                pooling_method=args.pooling_method,\n",
    "                                output_size=args.output_size)\n",
    "    \n",
    "    model.reset_parameters()\n",
    "\n",
    "    if args.parallel:\n",
    "        model = DataParallel(model, args.gpu_ids)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a28a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Projection, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_dim, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x) + self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vicreg_loss(embeddings1, embeddings2, lambda_var=25, lambda_cov=25, mu=1):\n",
    "    \"\"\"\n",
    "    Calculate the VICReg loss between two sets of embeddings with the correct handling of covariance differences.\n",
    "    \n",
    "    Args:\n",
    "    embeddings1, embeddings2 (torch.Tensor): Embeddings from two views, shape (batch_size, feature_dim).\n",
    "    lambda_var (float): Coefficient for the variance loss.\n",
    "    lambda_cov (float): Coefficient for the covariance loss.\n",
    "    mu (float): Coefficient for the invariance loss.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The total VICReg loss.\n",
    "    \"\"\"\n",
    "    # Invariance Loss\n",
    "    invariance_loss = F.mse_loss(embeddings1, embeddings2)\n",
    "\n",
    "    # Variance Loss\n",
    "    def variance_loss(embeddings1, embeddings2):\n",
    "        mean_embeddings1 = embeddings1.mean(dim=0)\n",
    "        mean_embeddings2 = embeddings2.mean(dim=0)\n",
    "        \n",
    "        std_dev1 = torch.sqrt((embeddings1 - mean_embeddings1).var(dim=0) + 1e-4)\n",
    "        std_dev2 = torch.sqrt((embeddings2 - mean_embeddings2).var(dim=0) + 1e-4)\n",
    "        \n",
    "        return torch.mean(torch.abs(F.relu(1 - std_dev1) - F.relu(1 - std_dev2)))\n",
    "\n",
    "    variance_loss_value = variance_loss(embeddings1, embeddings2)\n",
    "\n",
    "    # Covariance Loss\n",
    "    def covariance_loss(embeddings1, embeddings2):\n",
    "        batch_size, feature_dim = embeddings1.size()\n",
    "        \n",
    "        embeddings_centered1 = embeddings1 - embeddings1.mean(dim=0)\n",
    "        embeddings_centered2 = embeddings2 - embeddings2.mean(dim=0)\n",
    "        \n",
    "        covariance_matrix1 = torch.matmul(embeddings_centered1.T, embeddings_centered1) / (batch_size - 1)\n",
    "        covariance_matrix2 = torch.matmul(embeddings_centered2.T, embeddings_centered2) / (batch_size - 1)\n",
    "        \n",
    "        covariance_matrix1.fill_diagonal_(0)\n",
    "        covariance_matrix2.fill_diagonal_(0)\n",
    "        \n",
    "        cov_diff = torch.abs(covariance_matrix1.pow(2) - covariance_matrix2.pow(2))\n",
    "        return cov_diff.sum() / feature_dim\n",
    "\n",
    "    covariance_loss_value = covariance_loss(embeddings1, embeddings2)\n",
    "\n",
    "    total_loss = mu * invariance_loss + lambda_var * variance_loss_value + lambda_cov * covariance_loss_value\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53f69d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, model_1, model_2, mlp1, mlp2, aug1, aug2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "        self.mlp1 = mlp1\n",
    "        self.mlp2 = mlp2\n",
    "        self.aug1 = aug1\n",
    "        self.aug2 = aug2\n",
    "        \n",
    "    def get_embedding(self, data):\n",
    "        z, g = self.model_1(data)\n",
    "        z_pos, g_pos = self.model_2(data)\n",
    "        \n",
    "        z = self.mlp1(z)\n",
    "        g = self.mlp2(g)\n",
    "        \n",
    "        z_pos = self.mlp1(z_pos)\n",
    "        g_pos = self.mlp2(g_pos)\n",
    "\n",
    "        g = torch.cat((g, g_pos), 1)\n",
    "        z = torch.cat((z, z_pos), 1)\n",
    "\n",
    "        return g.detach(), z.detach()\n",
    "\n",
    "    def forward(self, data):\n",
    "        data1 = self.aug1(data.x, data.edge_index, data.y, data.pos, data.edge_attr,\n",
    "                          data.edge_attr_v2, data.batch, data.ptr)\n",
    "        data2 = self.aug2(data.x, data.edge_index, data.y, data.pos, data.edge_attr,\n",
    "                          data.edge_attr_v2, data.batch, data.ptr)\n",
    "        \n",
    "        # Structural features\n",
    "        z1, g1 = self.model_1(data1)\n",
    "        z2, g2 = self.model_1(data2)\n",
    "        \n",
    "        # Positional features\n",
    "        z1_pos, g1_pos = self.model_2(data1)\n",
    "        z2_pos, g2_pos = self.model_2(data2)\n",
    "        \n",
    "        h1, h2 = [self.mlp1(h) for h in [z1, z2]]\n",
    "        g1, g2 = [self.mlp2(g) for g in [g1, g2]]\n",
    "        \n",
    "        h1_pos, h2_pos = [self.mlp1(h_pos) for h_pos in [z1_pos, z2_pos]]\n",
    "        g1_pos, g2_pos = [self.mlp2(g_pos) for g_pos in [g1_pos, g2_pos]]\n",
    "        \n",
    "        h1 = torch.cat((h1, h1_pos), 1)\n",
    "        h2 = torch.cat((h2, h2_pos), 1)\n",
    "        g1 = torch.cat((g1, g1_pos), 1)\n",
    "        g2 = torch.cat((g2, g2_pos), 1)\n",
    "        \n",
    "        return h1, h2, g1, g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20eb92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder_model, dataloader, optimizer, device):\n",
    "    best = float(\"inf\")\n",
    "    cnt_wait = 0\n",
    "    best_t = 0\n",
    "    \n",
    "    loss_func = NTXentLoss(temperature=0.10)\n",
    "    \n",
    "    encoder_model.train()\n",
    "    epoch_loss = 0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if data.x is None:\n",
    "            num_nodes = data.batch.size(0)\n",
    "            data.x = torch.ones((num_nodes, 1), dtype=torch.float32, device=data.batch.device)\n",
    "\n",
    "        h1, h2, g1, g2 = encoder_model(data)\n",
    "        \n",
    "        embeddings = torch.cat((g1, g2))\n",
    "        \n",
    "        # The same index corresponds to a positive pair\n",
    "        indices = torch.arange(0, g1.size(0), device=device)\n",
    "        labels = torch.cat((indices, indices))\n",
    "        \n",
    "        reg_loss = vicreg_loss(h1, h2, lambda_var=24, lambda_cov=24, mu=1)\n",
    "        loss = loss_func(embeddings, labels) + 0.005*reg_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(\"Epoch: {0}, Loss: {1:0.4f}\".format(epoch, epoch_loss))\n",
    "\n",
    "        if epoch_loss < best:\n",
    "            best = epoch_loss\n",
    "            best_t = epoch\n",
    "            cnt_wait = 0\n",
    "            torch.save(encoder_model.state_dict(), './pkl/best_model_'+ args.dataset_name + tag + '.pkl')\n",
    "        else:\n",
    "            cnt_wait += 1\n",
    "\n",
    "        if cnt_wait == args.patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06b9ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder_model, dataset, dataloader, device):\n",
    "    encoder_model.eval()\n",
    "    data_list = []\n",
    "    for data in dataset:\n",
    "        data = data.to(device)\n",
    "        if data.x is None:\n",
    "            num_nodes = data.batch.size(0)\n",
    "            data.x = torch.ones((num_nodes, 1), dtype=torch.float32, device=data.batch.device)\n",
    "        graph_embeds, node_embeds = encoder_model.get_embedding(data)\n",
    "        data = Data(x=node_embeds, edge_index=data.edge_index, y=data.y, pos=data.pos, edge_attr=data.edge_attr,\n",
    "                    edge_attr_v2=data.edge_attr_v2, batch=data.batch, ptr=None)\n",
    "        data.graph_embeds = graph_embeds\n",
    "        data_list.append(data)\n",
    "\n",
    "    dataset.data.x = dataset.data.x.long()\n",
    "    dataset.data.y = torch.arange(len(dataset.data.y)).long()  # each graph is a unique class\n",
    "    train_dataset = data_list\n",
    "    val_dataset = data_list\n",
    "    test_dataset = data_list\n",
    "    \n",
    "    model = LogReg(hid_dim=data_list[0].graph_embeds.shape[1], out_dim=args.n_classes)\n",
    "    \n",
    "    # 2. create loader\n",
    "    train_loader = loader(train_dataset, args.batch_size, shuffle=True, num_workers=args.num_workers)\n",
    "    test_loader = loader(test_dataset, args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "    # additional parameter for SR dataset and training\n",
    "    args.input_size = 2\n",
    "    args.output_size = len(data_list)\n",
    "\n",
    "    # output argument to log file\n",
    "    log.info(f'Args: {dumps(vars(args), indent=4, sort_keys=True)}')\n",
    "    # get model\n",
    "    model.to(device)\n",
    "    pytorch_total_params = train_utils.count_parameters(model)\n",
    "    log.info(f'The total parameters of model :{[pytorch_total_params]}')\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.l2_wd)\n",
    "    best_test_acc = 0\n",
    "    start_outer = time.time()\n",
    "    for epoch in range(args.num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss = sr25_train(train_loader, model, optimizer, device=device, parallel=args.parallel)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        test_acc = sr25_test(test_loader, model, device=device, parallel=args.parallel)\n",
    "        if test_acc >= best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "        time_per_epoch = time.time() - start\n",
    "\n",
    "        log.info(f'Epoch: {epoch + 1:03d}, LR: {lr:7f}, Train Loss: {train_loss:.4f}, Test Acc: {test_acc:.4f}, '\n",
    "                 f'Best Test Acc: {best_test_acc:.4f}, Seconds: {time_per_epoch:.4f}')\n",
    "\n",
    "    time_average_epoch = time.time() - start_outer\n",
    "    log.info(f'Loss: {train_loss:.4f}, Best test: {best_test_acc:.4f}, Seconds/epoch: {time_average_epoch / (epoch + 1):.4f}')\n",
    "    \n",
    "    return train_loss, best_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d25fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/\" + args.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5ddb422",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path + '/processed'):\n",
    "    shutil.rmtree(path + '/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ad0b9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = SRDataset(path, pre_transform=T.Compose([edge_feature_transform]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70bf0d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum degree in the dataset\n",
    "max_degree = max([data.num_nodes for data in dataset])\n",
    "\n",
    "# Apply the OneHotDegree transform\n",
    "dataset.transform = T.OneHotDegree(max_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cf525f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.n_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "342bf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.input_size = dataset.num_features\n",
    "args.pos_size = args.pos_enc_dim\n",
    "args.output_size = args.hidden_size\n",
    "args.edge_attr_size =  dataset.edge_attr.shape[1]\n",
    "args.edge_attr_v2_size =  dataset.edge_attr_v2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db2533e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug1 = A.Identity()\n",
    "\n",
    "if args.feature_augmentation:\n",
    "    # Feature Augmentation\n",
    "    aug2 = A.RandomChoice([A.FeatureDropout(pf=0.1),\n",
    "                           A.FeatureMasking(pf=0.1),\n",
    "                           A.EdgeAttrMasking(pf=0.1)], 1)\n",
    "else:\n",
    "    # Structure Augmentation\n",
    "    aug2 = A.RandomChoice([A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                       A.NodeDropping(pn=0.1),\n",
    "                       A.EdgeRemoving(pe=0.1)], 1)\n",
    "\n",
    "model_1 = get_model(args)\n",
    "model_1.to(device)\n",
    "\n",
    "args.pos_attr = True\n",
    "args.input_size = args.pos_size\n",
    "model_2 = get_model(args)\n",
    "model_2.to(device)\n",
    "\n",
    "mlp1 = Projection(input_dim=args.hidden_size, output_dim=args.hidden_size)\n",
    "mlp2 = Projection(input_dim=args.hidden_size, output_dim=args.hidden_size)\n",
    "\n",
    "encoder_model = Encoder(model_1=model_1, model_2=model_2, mlp1=mlp1, mlp2=mlp2, aug1=aug1, aug2=aug2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42704cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T): 100%|████████████████████████████████████████| 10/10 [00:01<00:00,  7.10it/s, loss=3.84]\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(encoder_model.parameters(), lr=args.lr, weight_decay=args.l2_wd)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
    "with tqdm(total=10, desc='(T)') as pbar:\n",
    "    for epoch in range(1, 11):\n",
    "        loss = train(encoder_model, dataloader, optimizer, device)\n",
    "        pbar.set_postfix({'loss': loss})\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d28e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08.06.24 16:40:58] Args: {\n",
      "    \"JK\": \"sum\",\n",
      "    \"K\": 3,\n",
      "    \"aggr\": \"add\",\n",
      "    \"batch_size\": 32,\n",
      "    \"combine\": \"geometric\",\n",
      "    \"dataset_name\": \"sr25\",\n",
      "    \"drop_prob\": 0.6,\n",
      "    \"edge_attr_size\": 4,\n",
      "    \"edge_attr_v2_size\": 3,\n",
      "    \"factor\": 0.5,\n",
      "    \"feature_augmentation\": false,\n",
      "    \"gpu_ids\": [],\n",
      "    \"hidden_size\": 200,\n",
      "    \"input_size\": 2,\n",
      "    \"l2_wd\": 5e-06,\n",
      "    \"load_path\": null,\n",
      "    \"lr\": 0.001,\n",
      "    \"model_name\": \"KHopGNNConv\",\n",
      "    \"n_classes\": 1,\n",
      "    \"name\": \"KHopGNNConv_3_False\",\n",
      "    \"norm_type\": \"Batch\",\n",
      "    \"num_epochs\": 500,\n",
      "    \"num_layer\": 3,\n",
      "    \"num_workers\": 0,\n",
      "    \"output_size\": 15,\n",
      "    \"parallel\": false,\n",
      "    \"patience\": 20,\n",
      "    \"pooling_method\": \"sum\",\n",
      "    \"pos_attr\": true,\n",
      "    \"pos_enc_dim\": 6,\n",
      "    \"pos_size\": 6,\n",
      "    \"reprocess\": false,\n",
      "    \"residual\": true,\n",
      "    \"save_dir\": \"./save/sr25/KHopGNNConv_3_False-02\",\n",
      "    \"search\": false,\n",
      "    \"seed\": 2,\n",
      "    \"split\": 10,\n",
      "    \"train_eps\": false,\n",
      "    \"virtual_node\": false\n",
      "}\n",
      "[08.06.24 16:40:58] The total parameters of model :[401]\n",
      "[08.06.24 16:40:58] Epoch: 001, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0012\n",
      "[08.06.24 16:40:58] Epoch: 002, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 003, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 004, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 005, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 006, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 007, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 008, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 009, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 010, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 011, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 012, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 013, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 014, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 015, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 016, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 017, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 018, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 019, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 020, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 021, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 022, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 023, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 024, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 025, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 026, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 027, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 028, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 029, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 030, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 031, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 032, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 033, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 034, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 035, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 036, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 037, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 038, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 039, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 040, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 041, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 042, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 043, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 044, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 045, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 046, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 047, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 048, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 049, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 050, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 051, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 052, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 053, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 054, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 055, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 056, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 057, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 058, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 059, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 060, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 061, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 062, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 063, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 064, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 065, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 066, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 067, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 068, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 069, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 070, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 071, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 072, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 073, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 074, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 075, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 076, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 077, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 078, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 079, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 080, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:58] Epoch: 081, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 082, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 083, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 084, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 085, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 086, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 087, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 088, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 089, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:58] Epoch: 090, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 091, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 092, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 093, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 094, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 095, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:58] Epoch: 096, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 097, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 098, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 099, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 100, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 101, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 102, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 103, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 104, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 105, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 106, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 107, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 108, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 109, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 110, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 111, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 112, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 113, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 114, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 115, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 116, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 117, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 118, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 119, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 120, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 121, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 122, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 123, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 124, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 125, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 126, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 127, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 128, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 129, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 130, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 131, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 132, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 133, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 134, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 135, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 136, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 137, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 138, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 139, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 140, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 141, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 142, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 143, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 144, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 145, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 146, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 147, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 148, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 149, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 150, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 151, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 152, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 153, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 154, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 155, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 156, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 157, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 158, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 159, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 160, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 161, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 162, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 163, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 164, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 165, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 166, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 167, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 168, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 169, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 170, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 171, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 172, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 173, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 174, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 175, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 176, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 177, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 178, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 179, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 180, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 181, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 182, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 183, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 184, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 185, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 186, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 187, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 188, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 189, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 190, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 191, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 192, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 193, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 194, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 195, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 196, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 197, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 198, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 199, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 200, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 201, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 202, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 203, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 204, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 205, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 206, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 207, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 208, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 209, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 210, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 211, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 212, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 213, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 214, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 215, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 216, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 217, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 218, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 219, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 220, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 221, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 222, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 223, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 224, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 225, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 226, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 227, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 228, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 229, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 230, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 231, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 232, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 233, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 234, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 235, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 236, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 237, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 238, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 239, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 240, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 241, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 242, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 243, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 244, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 245, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08.06.24 16:40:59] Epoch: 246, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0018\n",
      "[08.06.24 16:40:59] Epoch: 247, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0013\n",
      "[08.06.24 16:40:59] Epoch: 248, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 249, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 250, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 251, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0012\n",
      "[08.06.24 16:40:59] Epoch: 252, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 253, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 254, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 255, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 256, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 257, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 258, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0012\n",
      "[08.06.24 16:40:59] Epoch: 259, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 260, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0012\n",
      "[08.06.24 16:40:59] Epoch: 261, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0011\n",
      "[08.06.24 16:40:59] Epoch: 262, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0012\n",
      "[08.06.24 16:40:59] Epoch: 263, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 264, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 265, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 266, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 267, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 268, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 269, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 270, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 271, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 272, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 273, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 274, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 275, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 276, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 277, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 278, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0011\n",
      "[08.06.24 16:40:59] Epoch: 279, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 280, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 281, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 282, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 283, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 284, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 285, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 286, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 287, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 288, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 289, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 290, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 291, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 292, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 293, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 294, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 295, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 296, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 297, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 298, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 299, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 300, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 301, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 302, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 303, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 304, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 305, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 306, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 307, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 308, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 309, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 310, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 311, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 312, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 313, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 314, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 315, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 316, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 317, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 318, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 319, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 320, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 321, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 322, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 323, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 324, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 325, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 326, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 327, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 328, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 329, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 330, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 331, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 332, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 333, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 334, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 335, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 336, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 337, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 338, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 339, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 340, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 341, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 342, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 343, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 344, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 345, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 346, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 347, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 348, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 349, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 350, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 351, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 352, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 353, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 354, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 355, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 356, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 357, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 358, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 359, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 360, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 361, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 362, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 363, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 364, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 365, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 366, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 367, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 368, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 369, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 370, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 371, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 372, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 373, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 374, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 375, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 376, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 377, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 378, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 379, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 380, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 381, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 382, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 383, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 384, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 385, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 386, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 387, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 388, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 389, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 390, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 391, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 392, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 393, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 394, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 395, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 396, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 397, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 398, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 399, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 400, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 401, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 402, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 403, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 404, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 405, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 406, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 407, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 408, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 409, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 410, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 411, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 412, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 413, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 414, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 415, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 416, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 417, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 418, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 419, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 420, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 421, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 422, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 423, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 424, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 425, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 426, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 427, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 428, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 429, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 430, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 431, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 432, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 433, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 434, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 435, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 436, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 437, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 438, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 439, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 440, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 441, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 442, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 443, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 444, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 445, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 446, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 447, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 448, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 449, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 450, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 451, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 452, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 453, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 454, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 455, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 456, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 457, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 458, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 459, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 460, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 461, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 462, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 463, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 464, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 465, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 466, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 467, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 468, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 469, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 470, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0007\n",
      "[08.06.24 16:40:59] Epoch: 471, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 472, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08.06.24 16:40:59] Epoch: 473, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0013\n",
      "[08.06.24 16:40:59] Epoch: 474, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0011\n",
      "[08.06.24 16:40:59] Epoch: 475, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 476, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 477, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 478, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 479, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 480, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0011\n",
      "[08.06.24 16:40:59] Epoch: 481, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 482, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 483, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 484, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 485, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 486, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 487, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 488, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 489, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 490, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 491, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 492, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 493, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 494, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 495, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 496, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0010\n",
      "[08.06.24 16:40:59] Epoch: 497, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 498, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0009\n",
      "[08.06.24 16:40:59] Epoch: 499, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Epoch: 500, LR: 0.001000, Train Loss: 0.0000, Test Acc: 1.0000, Best Test Acc: 1.0000, Seconds: 0.0008\n",
      "[08.06.24 16:40:59] Loss: 0.0000, Best test: 1.0000, Seconds/epoch: 0.0009\n",
      "[08.06.24 16:40:59] -------------------Print final result-------------------------\n",
      "[08.06.24 16:40:59] Test result: Mean: 1.0\n"
     ]
    }
   ],
   "source": [
    "_, test_accs = test(encoder_model, dataset, dataloader, device)\n",
    "\n",
    "log.info(\"-------------------Print final result-------------------------\")\n",
    "log.info(f\"Test result: Mean: {test_accs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de53a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
