{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4dc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import time\n",
    "from itertools import product\n",
    "from json import dumps\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "from torch_geometric.nn import DataParallel\n",
    "from torch_geometric.seed import seed_everything\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_metric_learning.losses import NTXentLoss, VICRegLoss\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "### importing OGB\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "\n",
    "import logging\n",
    "\n",
    "import train_utils\n",
    "from data_utils import extract_edge_attributes\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from layers.input_encoder import LinearEncoder, LinearEdgeEncoder\n",
    "from layers.layer_utils import make_gnn_layer\n",
    "from models.GraphClassification import GraphClassification\n",
    "from models.model_utils import make_GNN\n",
    "\n",
    "from embedding_evaluation import EmbeddingEvaluation\n",
    "\n",
    "import GCL.losses as L\n",
    "import GCL.augmentors as A\n",
    "from GCL.eval import get_split, SVMEvaluator, LREvaluator\n",
    "from GCL.models import DualBranchContrast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58bb1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--feature_augmentation'], dest='feature_augmentation', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, required=False, help='If true, feature augmentation.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(f'arguments for training and testing')\n",
    "parser.add_argument('--save_dir', type=str, default='./save', help='Base directory for saving information.')\n",
    "parser.add_argument('--seed', type=int, default=2, help='Random seed for reproducibility.')\n",
    "parser.add_argument('--dataset_name', type=str, default=\"ogbg-molpcba\",\n",
    "                    choices=(\"ogbg-molhiv\", \"ogbg-moltoxcast\", \"ogbg-moltox21\", \"ogbg-ppa\"), help='Name of dataset')\n",
    "parser.add_argument('--drop_prob', type=float, default=0.5,\n",
    "                    help='Probability of zeroing an activation in dropout layers.') \n",
    "parser.add_argument('--batch_size', type=int, default=32, help='Batch size per GPU. Scales automatically when \\\n",
    "                        multiple GPUs are available.')\n",
    "parser.add_argument(\"--parallel\", action=\"store_true\",\n",
    "                    help=\"If true, use DataParallel for multi-gpu training\")\n",
    "parser.add_argument('--num_workers', type=int, default=0, help='Number of worker.')\n",
    "parser.add_argument('--load_path', type=str, default=None, help='Path to load as a model checkpoint.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Learning rate.')\n",
    "parser.add_argument('--l2_wd', type=float, default=3e-3, help='L2 weight decay.')\n",
    "parser.add_argument('--num_epochs', type=int, default=100, help='Number of epochs.')\n",
    "parser.add_argument(\"--hidden_size\", type=int, default=200, help=\"Hidden size of the model\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"KHopGNNConv\",\n",
    "                    choices=(\"KHopGNNConv\"), help=\"Base GNN model\")\n",
    "parser.add_argument(\"--K\", type=int, default=3, help=\"Number of hop to consider\")\n",
    "parser.add_argument(\"--num_layer\", type=int, default=3, help=\"Number of layer for feature encoder\")\n",
    "parser.add_argument(\"--JK\", type=str, default=\"sum\", choices=(\"sum\", \"max\", \"mean\", \"attention\", \"last\", \"concat\"),\n",
    "                    help=\"Jumping knowledge method\")\n",
    "parser.add_argument(\"--residual\", default=True, action=\"store_true\", help=\"If true, use residual connection between each layer\")\n",
    "parser.add_argument(\"--virtual_node\", action=\"store_true\", default=False, \n",
    "                    help=\"If true, add virtual node information in each layer\")\n",
    "parser.add_argument('--feature', type=str, default=\"full\", help='full feature or simple feature')\n",
    "parser.add_argument('--filename', type=str, default=\"\", help='filename to output result (default: )')\n",
    "parser.add_argument(\"--train_eps\", action=\"store_true\", help=\"If true, the epsilon in GIN model is trainable\")\n",
    "parser.add_argument(\"--combine\", type=str, default=\"geometric\", choices=(\"attention\", \"geometric\"),\n",
    "                    help=\"Combine method in k-hop aggregation\")\n",
    "parser.add_argument(\"--pooling_method\", type=str, default=\"sum\", choices=(\"mean\", \"sum\", \"attention\"),\n",
    "                    help=\"Pooling method in graph classification\")\n",
    "parser.add_argument('--norm_type', type=str, default=\"Batch\",\n",
    "                    choices=(\"Batch\", \"Layer\", \"Instance\", \"GraphSize\", \"Pair\"),\n",
    "                    help=\"Normalization method in model\")\n",
    "parser.add_argument('--aggr', type=str, default=\"add\",\n",
    "                    help='Aggregation method in GNN layer, only works in GraphSAGE')\n",
    "parser.add_argument(\"--patience\", type=int, default=20, help=\"Patient epochs to wait before early stopping.\")\n",
    "parser.add_argument('--factor', type=float, default=0.5, help='Factor for reducing learning rate scheduler')\n",
    "parser.add_argument('--reprocess', action=\"store_true\", help='If true, reprocess the dataset')\n",
    "parser.add_argument('--search', action=\"store_true\", help='If true, search hyper-parameters')\n",
    "parser.add_argument(\"--pos_enc_dim\", type=int, default=6, help=\"Initial positional dim.\")\n",
    "parser.add_argument(\"--pos_attr\", type=bool, default=False, help=\"Positional attributes.\")\n",
    "parser.add_argument(\"--feature_augmentation\", type=bool, default=False, help=\"If true, feature augmentation.\")\n",
    "\n",
    "# parser = argparse.ArgumentParser(f'arguments for training and testing')\n",
    "# parser.add_argument('--save_dir', type=str, default='./save', help='Base directory for saving information.')\n",
    "# parser.add_argument('--seed', type=int, default=2, help='Random seed for reproducibility.') # 4 -> 93\n",
    "# parser.add_argument('--dataset_name', type=str, default=\"ogbg-moltoxcast\",\n",
    "#                     choices=(\"MUTAG\", \"PROTEINS\", \"PTC_MR\", \"IMDBBINARY\"), help='Name of dataset')\n",
    "# parser.add_argument('--drop_prob', type=float, default=0.6,\n",
    "#                     help='Probability of zeroing an activation in dropout layers.') # 0.5 -> 93\n",
    "# parser.add_argument('--batch_size', type=int, default=128, help='Batch size per GPU. Scales automatically when \\\n",
    "#                         multiple GPUs are available.') # 32 -> 93\n",
    "# parser.add_argument(\"--parallel\", action=\"store_true\",\n",
    "#                     help=\"If true, use DataParallel for multi-gpu training\")\n",
    "# parser.add_argument('--num_workers', type=int, default=0, help='Number of worker.')\n",
    "# parser.add_argument('--load_path', type=str, default=None, help='Path to load as a model checkpoint.')\n",
    "# parser.add_argument('--lr', type=float, default=0.001, help='Learning rate.') # 0.001 ->93\n",
    "# parser.add_argument('--l2_wd', type=float, default=3e-4, help='L2 weight decay.') # 3e-4 -> 93\n",
    "# parser.add_argument('--num_epochs', type=int, default=100, help='Number of epochs.') # 350 -> 93\n",
    "# parser.add_argument(\"--hidden_size\", type=int, default=128, help=\"Hidden size of the model\") # 128 -> 93\n",
    "# parser.add_argument(\"--embedding_size\", type=int, default=16, help=\"Output size of the logistic model\") # 16 -> 93\n",
    "# parser.add_argument(\"--model_name\", type=str, default=\"KHopGNNConv\",\n",
    "#                     choices=(\"KHopGNNConv\"), help=\"Base GNN model\")\n",
    "# parser.add_argument(\"--K\", type=int, default=2, help=\"Number of hop to consider\") # 2 -> 93\n",
    "# parser.add_argument(\"--num_layer\", type=int, default=2, help=\"Number of layer for feature encoder\") # 2 -> 93\n",
    "# parser.add_argument(\"--JK\", type=str, default=\"last\", choices=(\"sum\", \"max\", \"mean\", \"attention\", \"last\", \"concat\"),\n",
    "#                     help=\"Jumping knowledge method\") # sum -> 93\n",
    "# parser.add_argument(\"--residual\", default=True, action=\"store_true\", help=\"If true, use residual connection between each layer\")\n",
    "# parser.add_argument(\"--virtual_node\", action=\"store_true\", default=False, \n",
    "#                     help=\"If true, add virtual node information in each layer\")\n",
    "# parser.add_argument('--feature', type=str, default=\"full\", help='full feature or simple feature')\n",
    "# parser.add_argument('--filename', type=str, default=\"\", help='filename to output result (default: )')\n",
    "# parser.add_argument(\"--train_eps\", action=\"store_true\", help=\"If true, the epsilon in GIN model is trainable\")\n",
    "# parser.add_argument(\"--combine\", type=str, default=\"geometric\", choices=(\"attention\", \"geometric\"),\n",
    "#                     help=\"Combine method in k-hop aggregation\") # geometric -> 93\n",
    "# parser.add_argument(\"--pooling_method\", type=str, default=\"sum\", choices=(\"mean\", \"sum\", \"attention\"),\n",
    "#                     help=\"Pooling method in graph classification\") # sum -> 93\n",
    "# parser.add_argument('--norm_type', type=str, default=\"Batch\",\n",
    "#                     choices=(\"Batch\", \"Layer\", \"Instance\", \"GraphSize\", \"Pair\"),\n",
    "#                     help=\"Normalization method in model\") # Batch -> 93\n",
    "# parser.add_argument('--aggr', type=str, default=\"add\",\n",
    "#                     help='Aggregation method in GNN layer, only works in GraphSAGE')\n",
    "# parser.add_argument(\"--patience\", type=int, default=20, help=\"Patient epochs to wait before early stopping.\")\n",
    "# parser.add_argument('--factor', type=float, default=0.5, help='Factor for reducing learning rate scheduler')\n",
    "# parser.add_argument('--reprocess', action=\"store_true\", help='If true, reprocess the dataset')\n",
    "# parser.add_argument('--search', action=\"store_true\", help='If true, search hyper-parameters')\n",
    "# parser.add_argument(\"--pos_enc_dim\", type=int, default=6, help=\"Initial positional dim.\") # 6 -> 93\n",
    "# parser.add_argument(\"--pos_attr\", type=bool, default=False, help=\"Positional attributes.\")\n",
    "# parser.add_argument(\"--feature_augmentation\", type=bool, default=True, help=\"If true, feature augmentation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ced7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc90192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.name = args.model_name + \"_\" + str(args.K) + \"_\" + str(args.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cfbbe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging and devices\n",
    "args.save_dir = train_utils.get_save_dir(args.save_dir, args.name, type=args.dataset_name)\n",
    "log = train_utils.get_logger(args.save_dir, args.name)\n",
    "device, args.gpu_ids = train_utils.get_available_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5ea237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08.07.24 12:48:23] Using single-gpu training\n"
     ]
    }
   ],
   "source": [
    "if len(args.gpu_ids) > 1 and args.parallel:\n",
    "    log.info(f'Using multi-gpu training')\n",
    "    args.parallel = True\n",
    "    loader = DataListLoader\n",
    "    args.batch_size *= max(1, len(args.gpu_ids))\n",
    "else:\n",
    "    log.info(f'Using single-gpu training')\n",
    "    args.parallel = False\n",
    "    loader = DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c54f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08.07.24 12:48:23] Using random seed 2...\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "seed = args.seed\n",
    "log.info(f'Using random seed {seed}...')\n",
    "seed_everything(seed)\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c291b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_feature_transform(g):\n",
    "    return extract_edge_attributes(g, args.pos_enc_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "782b2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3529c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, hid_dim, out_dim):\n",
    "        super(LogReg, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(hid_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = self.fc(x)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c818ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "reg_criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55ee85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    layer = make_gnn_layer(args)\n",
    "    init_emb = LinearEncoder(args.input_size, args.hidden_size, pos_attr=args.pos_attr)\n",
    "    init_edge_attr_emb = LinearEdgeEncoder(args.edge_attr_size, args.hidden_size, edge_attr=True)\n",
    "    init_edge_attr_v2_emb = LinearEdgeEncoder(args.edge_attr_v2_size, args.hidden_size, edge_attr=False)\n",
    "    \n",
    "    GNNModel = make_GNN(args)\n",
    "    \n",
    "    gnn = GNNModel(\n",
    "        num_layer=args.num_layer,\n",
    "        gnn_layer=layer,\n",
    "        JK=args.JK,\n",
    "        norm_type=args.norm_type,\n",
    "        init_emb=init_emb,\n",
    "        init_edge_attr_emb=init_edge_attr_emb,\n",
    "        init_edge_attr_v2_emb=init_edge_attr_v2_emb,\n",
    "        residual=args.residual,\n",
    "        virtual_node=args.virtual_node,\n",
    "        drop_prob=args.drop_prob)\n",
    "\n",
    "    model = GraphClassification(embedding_model=gnn,\n",
    "                                pooling_method=args.pooling_method,\n",
    "                                output_size=args.output_size)\n",
    "    \n",
    "    model.reset_parameters()\n",
    "\n",
    "    if args.parallel:\n",
    "        model = DataParallel(model, args.gpu_ids)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06b31cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Projection, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_dim, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x) + self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628301be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vicreg_loss(embeddings1, embeddings2, lambda_var=25, lambda_cov=25, mu=1):\n",
    "    \"\"\"\n",
    "    Calculate the VICReg loss between two sets of embeddings with the correct handling of covariance differences.\n",
    "    \n",
    "    Args:\n",
    "    embeddings1, embeddings2 (torch.Tensor): Embeddings from two views, shape (batch_size, feature_dim).\n",
    "    lambda_var (float): Coefficient for the variance loss.\n",
    "    lambda_cov (float): Coefficient for the covariance loss.\n",
    "    mu (float): Coefficient for the invariance loss.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The total VICReg loss.\n",
    "    \"\"\"\n",
    "    # Invariance Loss\n",
    "    invariance_loss = F.mse_loss(embeddings1, embeddings2)\n",
    "\n",
    "    # Variance Loss\n",
    "    def variance_loss(embeddings1, embeddings2):\n",
    "        mean_embeddings1 = embeddings1.mean(dim=0)\n",
    "        mean_embeddings2 = embeddings2.mean(dim=0)\n",
    "        \n",
    "        std_dev1 = torch.sqrt((embeddings1 - mean_embeddings1).var(dim=0) + 1e-4)\n",
    "        std_dev2 = torch.sqrt((embeddings2 - mean_embeddings2).var(dim=0) + 1e-4)\n",
    "        \n",
    "        return torch.mean(torch.abs(F.relu(1 - std_dev1) - F.relu(1 - std_dev2)))\n",
    "\n",
    "    variance_loss_value = variance_loss(embeddings1, embeddings2)\n",
    "\n",
    "    # Covariance Loss\n",
    "    def covariance_loss(embeddings1, embeddings2):\n",
    "        batch_size, feature_dim = embeddings1.size()\n",
    "        \n",
    "        embeddings_centered1 = embeddings1 - embeddings1.mean(dim=0)\n",
    "        embeddings_centered2 = embeddings2 - embeddings2.mean(dim=0)\n",
    "        \n",
    "        covariance_matrix1 = torch.matmul(embeddings_centered1.T, embeddings_centered1) / (batch_size - 1)\n",
    "        covariance_matrix2 = torch.matmul(embeddings_centered2.T, embeddings_centered2) / (batch_size - 1)\n",
    "        \n",
    "        covariance_matrix1.fill_diagonal_(0)\n",
    "        covariance_matrix2.fill_diagonal_(0)\n",
    "        \n",
    "        cov_diff = torch.abs(covariance_matrix1.pow(2) - covariance_matrix2.pow(2))\n",
    "        return cov_diff.sum() / feature_dim\n",
    "\n",
    "    covariance_loss_value = covariance_loss(embeddings1, embeddings2)\n",
    "\n",
    "    total_loss = mu * invariance_loss + lambda_var * variance_loss_value + lambda_cov * covariance_loss_value\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3f17bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, model_1, model_2, mlp1, mlp2, aug1, aug2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "        self.mlp1 = mlp1\n",
    "        self.mlp2 = mlp2\n",
    "        self.aug1 = aug1\n",
    "        self.aug2 = aug2\n",
    "        \n",
    "    def get_embedding(self, data):\n",
    "        z, g = self.model_1(data)\n",
    "        z_pos, g_pos = self.model_2(data)\n",
    "        \n",
    "        z = self.mlp1(z)\n",
    "        g = self.mlp2(g)\n",
    "        \n",
    "        z_pos = self.mlp1(z_pos)\n",
    "        g_pos = self.mlp2(g_pos)\n",
    "\n",
    "        g = torch.cat((g, g_pos), 1)\n",
    "        z = torch.cat((z, z_pos), 1)\n",
    "\n",
    "        return g.detach(), z.detach()\n",
    "\n",
    "    def forward(self, data):\n",
    "        data1 = self.aug1(data.x, data.edge_index, data.y, data.pos, data.edge_attr,\n",
    "                          data.edge_attr_v2, data.batch, data.ptr)\n",
    "        data2 = self.aug2(data.x, data.edge_index, data.y, data.pos, data.edge_attr,\n",
    "                          data.edge_attr_v2, data.batch, data.ptr)\n",
    "        \n",
    "        # Structural features\n",
    "        z1, g1 = self.model_1(data1)\n",
    "        z2, g2 = self.model_1(data2)\n",
    "        \n",
    "        # Positional features\n",
    "        z1_pos, g1_pos = self.model_2(data1)\n",
    "        z2_pos, g2_pos = self.model_2(data2)\n",
    "        \n",
    "        h1, h2 = [self.mlp1(h) for h in [z1, z2]]\n",
    "        g1, g2 = [self.mlp2(g) for g in [g1, g2]]\n",
    "        \n",
    "        h1_pos, h2_pos = [self.mlp1(h_pos) for h_pos in [z1_pos, z2_pos]]\n",
    "        g1_pos, g2_pos = [self.mlp2(g_pos) for g_pos in [g1_pos, g2_pos]]\n",
    "        \n",
    "        h1 = torch.cat((h1, h1_pos), 1)\n",
    "        h2 = torch.cat((h2, h2_pos), 1)\n",
    "        g1 = torch.cat((g1, g1_pos), 1)\n",
    "        g2 = torch.cat((g2, g2_pos), 1)\n",
    "        \n",
    "        return h1, h2, g1, g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2429732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder_model, dataloader, optimizer, device):\n",
    "    best = float(\"inf\")\n",
    "    cnt_wait = 0\n",
    "    best_t = 0\n",
    "    \n",
    "    loss_func = NTXentLoss(temperature=0.10)\n",
    "    \n",
    "    encoder_model.train()\n",
    "    epoch_loss = 0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if data.x is None:\n",
    "            num_nodes = data.batch.size(0)\n",
    "            data.x = torch.ones((num_nodes, 1), dtype=torch.float32, device=data.batch.device)\n",
    "\n",
    "        h1, h2, g1, g2 = encoder_model(data)\n",
    "        \n",
    "        embeddings = torch.cat((g1, g2))\n",
    "        \n",
    "        # The same index corresponds to a positive pair\n",
    "        indices = torch.arange(0, g1.size(0), device=device)\n",
    "        labels = torch.cat((indices, indices))\n",
    "        \n",
    "        reg_loss = vicreg_loss(h1, h2, lambda_var=24, lambda_cov=24, mu=1)\n",
    "        loss = loss_func(embeddings, labels) + 0.005*reg_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(\"Epoch: {0}, Loss: {1:0.4f}\".format(epoch, epoch_loss))\n",
    "\n",
    "        if epoch_loss < best:\n",
    "            best = epoch_loss\n",
    "            best_t = epoch\n",
    "            cnt_wait = 0\n",
    "            torch.save(encoder_model.state_dict(), './pkl/best_model_'+ args.dataset_name + tag + '.pkl')\n",
    "        else:\n",
    "            cnt_wait += 1\n",
    "\n",
    "        if cnt_wait == args.patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f164d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder_model, dataloader, dataset, loader, args, device, log):\n",
    "    encoder_model.eval()\n",
    "    x = []\n",
    "    y = []\n",
    "    data_list = []\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        if data.x is None:\n",
    "            num_nodes = data.batch.size(0)\n",
    "            data.x = torch.ones((num_nodes, 1), dtype=torch.float32, device=data.batch.device)\n",
    "        graph_embeds, node_embeds = encoder_model.get_embedding(data)\n",
    "        x.append(graph_embeds)\n",
    "        y.append(data.y)\n",
    "        \n",
    "    x = torch.cat(x, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "    \n",
    "    split_idx = dataset.get_idx_split()\n",
    "    embeddings = x.detach().numpy()\n",
    "    labels = y.detach().numpy()\n",
    "\n",
    "    train_data_list = []\n",
    "    train_y_list = []\n",
    "    valid_data_list = []\n",
    "    valid_y_list = []\n",
    "    test_data_list = []\n",
    "    test_y_list = []\n",
    "    \n",
    "    ### automatic evaluator. takes dataset name as input\n",
    "    evaluator = Evaluator(args.dataset_name)\n",
    "    \n",
    "    test_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=None)\n",
    "    \n",
    "    for k_id, (train_val_index, test_index) in enumerate(kf.split(y)):\n",
    "        test_dataset = [x[int(i)] for i in list(test_index)]\n",
    "        train_index, val_index = train_test_split(train_val_index, test_size=0.2, random_state=None)\n",
    "        \n",
    "        train_dataset = [x[int(i)] for i in list(train_index)]\n",
    "        val_dataset = [x[int(i)] for i in list(val_index)]\n",
    "        \n",
    "        train_y_list = [y[int(i)] for i in list(train_index)]\n",
    "        valid_y_list = [y[int(i)] for i in list(val_index)]\n",
    "        test_y_list = [y[int(i)] for i in list(test_index)]\n",
    "        \n",
    "        train_emb = np.stack(train_dataset, axis=0)\n",
    "        train_y = np.stack(train_y_list, axis=0)\n",
    "\n",
    "        val_emb = np.stack(val_dataset, axis=0)\n",
    "        val_y = np.stack(valid_y_list, axis=0)\n",
    "\n",
    "        test_emb = np.stack(test_dataset, axis=0)\n",
    "        test_y = np.stack(test_y_list, axis=0)\n",
    "        \n",
    "        ee = EmbeddingEvaluation(LogisticRegression(dual=False, fit_intercept=True, solver='lbfgs', max_iter=5000),\n",
    "                            evaluator, dataset.task_type, dataset.num_tasks, device, params_dict=None,\n",
    "                            param_search=True)\n",
    "    \n",
    "        train_score, val_score, test_score = ee.embedding_evaluation(encoder_model, train_emb, train_y, \n",
    "                                                                 val_emb, val_y, test_emb, test_y)\n",
    "        \n",
    "        test_acc.append(test_score)\n",
    "        val_acc.append(val_score)\n",
    "\n",
    "    test_acc_mean = np.mean(test_acc, axis=0)\n",
    "    test_acc_std = np.std(test_acc, axis=0)\n",
    "    return test_acc_mean*100, test_acc_std*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b917dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### automatic dataloading and splitting\n",
    "dataset = PygGraphPropPredDataset(name = args.dataset_name, root = './data/', pre_transform=T.Compose([edge_feature_transform]))\n",
    "\n",
    "if args.feature == 'full':\n",
    "    pass \n",
    "elif args.feature == 'simple':\n",
    "    print('using simple feature')\n",
    "    # only retain the top two node/edge features\n",
    "    dataset.data.x = dataset.data.x[:,:2]\n",
    "    dataset.data.edge_attr = dataset.data.edge_attr[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "581c0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = torch.unique(dataset.y)\n",
    "args.n_classes = dataset.num_tasks # len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5864d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.input_size = dataset.num_node_features\n",
    "args.pos_size = args.pos_enc_dim\n",
    "args.output_size = args.hidden_size\n",
    "args.edge_attr_size =  dataset.edge_attr.shape[1]\n",
    "args.edge_attr_v2_size =  dataset.edge_attr_v2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f41cdfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug1 = A.Identity()\n",
    "\n",
    "if args.feature_augmentation:\n",
    "    # Feature Augmentation\n",
    "    aug2 = A.RandomChoice([A.FeatureDropout(pf=0.1),\n",
    "                           A.FeatureMasking(pf=0.1),\n",
    "                           A.EdgeAttrMasking(pf=0.1)], 1)\n",
    "else:\n",
    "    # Structure Augmentation\n",
    "    aug2 = A.RandomChoice([A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                       A.NodeDropping(pn=0.1),\n",
    "                       A.EdgeRemoving(pe=0.1)], 1)\n",
    "\n",
    "model_1 = get_model(args)\n",
    "model_1.to(device)\n",
    "\n",
    "args.pos_attr = True\n",
    "args.input_size = args.pos_size\n",
    "model_2 = get_model(args)\n",
    "model_2.to(device)\n",
    "\n",
    "mlp1 = Projection(input_dim=args.hidden_size, output_dim=args.hidden_size)\n",
    "mlp2 = Projection(input_dim=args.hidden_size, output_dim=args.hidden_size)\n",
    "\n",
    "encoder_model = Encoder(model_1=model_1, model_2=model_2, mlp1=mlp1, mlp2=mlp2, aug1=aug1, aug2=aug2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c34ad8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  10%|████                                     | 1/10 [00:02<00:24,  2.71s/it, loss=71.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  20%|████████▏                                | 2/10 [00:05<00:21,  2.70s/it, loss=29.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  30%|████████████▉                              | 3/10 [00:08<00:19,  2.83s/it, loss=21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  40%|████████████████▍                        | 4/10 [00:11<00:17,  2.86s/it, loss=16.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  50%|████████████████████▌                    | 5/10 [00:13<00:13,  2.78s/it, loss=14.5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  60%|████████████████████████▌                | 6/10 [00:16<00:10,  2.72s/it, loss=12.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  70%|████████████████████████████▋            | 7/10 [00:18<00:07,  2.63s/it, loss=11.8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  80%|████████████████████████████████▊        | 8/10 [00:21<00:05,  2.62s/it, loss=10.6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  90%|████████████████████████████████████▉    | 9/10 [00:24<00:02,  2.80s/it, loss=10.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T): 100%|█████████████████████████████████████████| 10/10 [00:27<00:00,  2.72s/it, loss=9.7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(encoder_model.parameters(), lr=args.lr, weight_decay=args.l2_wd)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
    "with tqdm(total=10, desc='(T)') as pbar: # 1000\n",
    "    for epoch in range(1, 11): # 1001\n",
    "        loss = train(encoder_model, dataloader, optimizer, device)\n",
    "        pbar.set_postfix({'loss': loss})\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b32a860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has NaNs ... ignoring them\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process LokyProcess-2:\n",
      "Process LokyProcess-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 456, in _process_worker\n",
      "    mem_usage = _get_memory_usage(pid, force_gc=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 108, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 466, in _process_worker\n",
      "    with worker_exit_lock:\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/backend/synchronize.py\", line 112, in __enter__\n",
      "    return self._semlock.acquire()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Process LokyProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 456, in _process_worker\n",
      "    mem_usage = _get_memory_usage(pid, force_gc=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 108, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "Process LokyProcess-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 456, in _process_worker\n",
      "    mem_usage = _get_memory_usage(pid, force_gc=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wij023/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py\", line 108, in _get_memory_usage\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_mean, test_std = test(encoder_model, dataloader, dataset, loader, args, device, log)\n",
    "\n",
    "print(f'test acc mean = {test_mean:.4f} ± {test_std:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37574b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
