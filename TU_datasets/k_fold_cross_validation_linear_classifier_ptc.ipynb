{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021e9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import time\n",
    "from itertools import product\n",
    "from json import dumps\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "from torch_geometric.nn import DataParallel\n",
    "from torch_geometric.seed import seed_everything\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data import DataLoader, Data\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_metric_learning.losses import NTXentLoss, VICRegLoss\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "import train_utils\n",
    "from data_utils import extract_edge_attributes\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from layers.input_encoder import LinearEncoder, LinearEdgeEncoder\n",
    "from layers.layer_utils import make_gnn_layer\n",
    "from models.GraphClassification import GraphClassification\n",
    "from models.model_utils import make_GNN\n",
    "\n",
    "import GCL.losses as L\n",
    "import GCL.augmentors as A\n",
    "from GCL.eval import get_split, SVMEvaluator, LREvaluator\n",
    "from GCL.models import DualBranchContrast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4f8966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--feature_augmentation'], dest='feature_augmentation', nargs=None, const=None, default=True, type=<class 'bool'>, choices=None, required=False, help='If true, feature augmentation.', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(f'arguments for training and testing')\n",
    "parser.add_argument('--save_dir', type=str, default='./save', help='Base directory for saving information.')\n",
    "parser.add_argument('--seed', type=int, default=2, help='Random seed for reproducibility.')\n",
    "parser.add_argument('--dataset_name', type=str, default=\"PTC_MR\",\n",
    "                    choices=(\"MUTAG\", \"PROTEINS\", \"PTC_MR\", \"IMDBBINARY\"), help='Name of dataset')\n",
    "parser.add_argument('--drop_prob', type=float, default=0.5,\n",
    "                    help='Probability of zeroing an activation in dropout layers.') \n",
    "parser.add_argument('--batch_size', type=int, default=256, help='Batch size per GPU. Scales automatically when \\\n",
    "                        multiple GPUs are available.')\n",
    "parser.add_argument(\"--parallel\", action=\"store_true\",\n",
    "                    help=\"If true, use DataParallel for multi-gpu training\")\n",
    "parser.add_argument('--num_workers', type=int, default=0, help='Number of worker.')\n",
    "parser.add_argument('--load_path', type=str, default=None, help='Path to load as a model checkpoint.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Learning rate.')\n",
    "parser.add_argument('--l2_wd', type=float, default=3e-4, help='L2 weight decay.')\n",
    "parser.add_argument('--num_epochs', type=int, default=100, help='Number of epochs.')\n",
    "parser.add_argument(\"--hidden_size\", type=int, default=128, help=\"Hidden size of the model\")\n",
    "parser.add_argument(\"--embedding_size\", type=int, default=16, help=\"Output size of the logistic model\")\n",
    "parser.add_argument(\"--model_name\", type=str, default=\"KHopGNNConv\",\n",
    "                    choices=(\"KHopGNNConv\"), help=\"Base GNN model\")\n",
    "parser.add_argument(\"--K\", type=int, default=2, help=\"Number of hop to consider\")\n",
    "parser.add_argument(\"--num_layer\", type=int, default=2, help=\"Number of layer for feature encoder\")\n",
    "parser.add_argument(\"--JK\", type=str, default=\"sum\", choices=(\"sum\", \"max\", \"mean\", \"attention\", \"last\", \"concat\"),\n",
    "                    help=\"Jumping knowledge method\")\n",
    "parser.add_argument(\"--residual\", default=True, action=\"store_true\", help=\"If true, use residual connection between each layer\")\n",
    "parser.add_argument(\"--virtual_node\", action=\"store_true\", default=False, \n",
    "                    help=\"If true, add virtual node information in each layer\")\n",
    "parser.add_argument(\"--eps\", type=float, default=0., help=\"Initial epsilon in GIN\")\n",
    "parser.add_argument(\"--train_eps\", action=\"store_true\", help=\"If true, the epsilon in GIN model is trainable\")\n",
    "parser.add_argument(\"--combine\", type=str, default=\"geometric\", choices=(\"attention\", \"geometric\"),\n",
    "                    help=\"Combine method in k-hop aggregation\")\n",
    "parser.add_argument(\"--pooling_method\", type=str, default=\"sum\", choices=(\"mean\", \"sum\", \"attention\"),\n",
    "                    help=\"Pooling method in graph classification\")\n",
    "parser.add_argument('--norm_type', type=str, default=\"Batch\",\n",
    "                    choices=(\"Batch\", \"Layer\", \"Instance\", \"GraphSize\", \"Pair\"),\n",
    "                    help=\"Normalization method in model\")\n",
    "parser.add_argument('--aggr', type=str, default=\"add\",\n",
    "                    help='Aggregation method in GNN layer, only works in GraphSAGE')\n",
    "parser.add_argument(\"--patience\", type=int, default=20, help=\"Patient epochs to wait before early stopping.\")\n",
    "parser.add_argument('--factor', type=float, default=0.5, help='Factor for reducing learning rate scheduler')\n",
    "parser.add_argument('--reprocess', action=\"store_true\", help='If true, reprocess the dataset')\n",
    "parser.add_argument('--search', action=\"store_true\", help='If true, search hyper-parameters')\n",
    "parser.add_argument(\"--pos_enc_dim\", type=int, default=6, help=\"Initial positional dim.\")\n",
    "parser.add_argument(\"--pos_attr\", type=bool, default=False, help=\"Positional attributes.\")\n",
    "parser.add_argument(\"--feature_augmentation\", type=bool, default=True, help=\"If true, feature augmentation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6771bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eee13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.name = args.model_name + \"_\" + str(args.K) + \"_\" + str(args.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging and devices\n",
    "args.save_dir = train_utils.get_save_dir(args.save_dir, args.name, type=args.dataset_name)\n",
    "log = train_utils.get_logger(args.save_dir, args.name)\n",
    "device, args.gpu_ids = train_utils.get_available_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54fd4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01.27.25 21:52:12] Using single-gpu training\n"
     ]
    }
   ],
   "source": [
    "if len(args.gpu_ids) > 1 and args.parallel:\n",
    "    log.info(f'Using multi-gpu training')\n",
    "    args.parallel = True\n",
    "    loader = DataListLoader\n",
    "    args.batch_size *= max(1, len(args.gpu_ids))\n",
    "else:\n",
    "    log.info(f'Using single-gpu training')\n",
    "    args.parallel = False\n",
    "    loader = DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9b8739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01.27.25 21:52:12] Using random seed 2...\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "seed = args.seed\n",
    "log.info(f'Using random seed {seed}...')\n",
    "seed_everything(seed)\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2fe8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_feature_transform(g):\n",
    "    return extract_edge_attributes(g, args.pos_enc_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "224095d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = str(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f0bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    layer = make_gnn_layer(args)\n",
    "    init_emb = LinearEncoder(args.input_size, args.hidden_size, pos_attr=args.pos_attr)\n",
    "    init_edge_attr_emb = LinearEdgeEncoder(args.edge_attr_size, args.hidden_size, edge_attr=True)\n",
    "    init_edge_attr_v2_emb = LinearEdgeEncoder(args.edge_attr_v2_size, args.hidden_size, edge_attr=False)\n",
    "    \n",
    "    GNNModel = make_GNN(args)\n",
    "    \n",
    "    gnn = GNNModel(\n",
    "        num_layer=args.num_layer,\n",
    "        gnn_layer=layer,\n",
    "        JK=args.JK,\n",
    "        norm_type=args.norm_type,\n",
    "        init_emb=init_emb,\n",
    "        init_edge_attr_emb=init_edge_attr_emb,\n",
    "        init_edge_attr_v2_emb=init_edge_attr_v2_emb,\n",
    "        residual=args.residual,\n",
    "        virtual_node=args.virtual_node,\n",
    "        drop_prob=args.drop_prob)\n",
    "\n",
    "    model = GraphClassification(embedding_model=gnn,\n",
    "                                pooling_method=args.pooling_method,\n",
    "                                output_size=args.output_size)\n",
    "    \n",
    "    model.reset_parameters()\n",
    "\n",
    "    if args.parallel:\n",
    "        model = DataParallel(model, args.gpu_ids)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd8282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Projection, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(output_dim, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x) + self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01693900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vicreg_loss(embeddings1, embeddings2, lambda_var=25, lambda_cov=25, mu=1):\n",
    "    \"\"\"\n",
    "    Calculate the VICReg loss between two sets of embeddings with the correct handling of covariance differences.\n",
    "    \n",
    "    Args:\n",
    "    embeddings1, embeddings2 (torch.Tensor): Embeddings from two views, shape (batch_size, feature_dim).\n",
    "    lambda_var (float): Coefficient for the variance loss.\n",
    "    lambda_cov (float): Coefficient for the covariance loss.\n",
    "    mu (float): Coefficient for the invariance loss.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The total VICReg loss.\n",
    "    \"\"\"\n",
    "    # Invariance Loss\n",
    "    invariance_loss = F.mse_loss(embeddings1, embeddings2)\n",
    "\n",
    "    # Variance Loss\n",
    "    def variance_loss(embeddings1, embeddings2):\n",
    "        mean_embeddings1 = embeddings1.mean(dim=0)\n",
    "        mean_embeddings2 = embeddings2.mean(dim=0)\n",
    "        \n",
    "        std_dev1 = torch.sqrt((embeddings1 - mean_embeddings1).var(dim=0) + 1e-4)\n",
    "        std_dev2 = torch.sqrt((embeddings2 - mean_embeddings2).var(dim=0) + 1e-4)\n",
    "        \n",
    "        return torch.mean(torch.abs(F.relu(1 - std_dev1) - F.relu(1 - std_dev2)))\n",
    "\n",
    "    variance_loss_value = variance_loss(embeddings1, embeddings2)\n",
    "\n",
    "    # Covariance Loss\n",
    "    def covariance_loss(embeddings1, embeddings2):\n",
    "        batch_size, feature_dim = embeddings1.size()\n",
    "        \n",
    "        embeddings_centered1 = embeddings1 - embeddings1.mean(dim=0)\n",
    "        embeddings_centered2 = embeddings2 - embeddings2.mean(dim=0)\n",
    "        \n",
    "        covariance_matrix1 = torch.matmul(embeddings_centered1.T, embeddings_centered1) / (batch_size - 1)\n",
    "        covariance_matrix2 = torch.matmul(embeddings_centered2.T, embeddings_centered2) / (batch_size - 1)\n",
    "        \n",
    "        covariance_matrix1.fill_diagonal_(0)\n",
    "        covariance_matrix2.fill_diagonal_(0)\n",
    "        \n",
    "        cov_diff = torch.abs(covariance_matrix1.pow(2) - covariance_matrix2.pow(2))\n",
    "        return cov_diff.sum() / feature_dim\n",
    "\n",
    "    covariance_loss_value = covariance_loss(embeddings1, embeddings2)\n",
    "\n",
    "    total_loss = mu * invariance_loss + lambda_var * variance_loss_value + lambda_cov * covariance_loss_value\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed99714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, model_1, model_2, mlp1, mlp2, aug1, aug2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "        self.mlp1 = mlp1\n",
    "        self.mlp2 = mlp2\n",
    "        self.aug1 = aug1\n",
    "        self.aug2 = aug2\n",
    "        \n",
    "    def get_embedding(self, data):\n",
    "        z, g = self.model_1(data)\n",
    "        z_pos, g_pos = self.model_2(data)\n",
    "        \n",
    "        z = self.mlp1(z)\n",
    "        g = self.mlp2(g)\n",
    "        \n",
    "        z_pos = self.mlp1(z_pos)\n",
    "        g_pos = self.mlp2(g_pos)\n",
    "\n",
    "        g = torch.cat((g, g_pos), 1)\n",
    "        z = torch.cat((z, z_pos), 1)\n",
    "\n",
    "        return g.detach(), z.detach()\n",
    "\n",
    "    def forward(self, data):\n",
    "        data1 = self.aug1(data.x, data.edge_index, data.y, data.pos, data.edge_attr,\n",
    "                          data.edge_attr_v2, data.batch, data.ptr)\n",
    "        data2 = self.aug2(data.x, data.edge_index, data.y, data.pos, data.edge_attr,\n",
    "                          data.edge_attr_v2, data.batch, data.ptr)\n",
    "        \n",
    "        # Structural features\n",
    "        z1, g1 = self.model_1(data1)\n",
    "        z2, g2 = self.model_1(data2)\n",
    "        \n",
    "        # Positional features\n",
    "        z1_pos, g1_pos = self.model_2(data1)\n",
    "        z2_pos, g2_pos = self.model_2(data2)\n",
    "        \n",
    "        h1, h2 = [self.mlp1(h) for h in [z1, z2]]\n",
    "        g1, g2 = [self.mlp2(g) for g in [g1, g2]]\n",
    "        \n",
    "        h1_pos, h2_pos = [self.mlp1(h_pos) for h_pos in [z1_pos, z2_pos]]\n",
    "        g1_pos, g2_pos = [self.mlp2(g_pos) for g_pos in [g1_pos, g2_pos]]\n",
    "        \n",
    "        h1 = torch.cat((h1, h1_pos), 1)\n",
    "        h2 = torch.cat((h2, h2_pos), 1)\n",
    "        g1 = torch.cat((g1, g1_pos), 1)\n",
    "        g2 = torch.cat((g2, g2_pos), 1)\n",
    "        \n",
    "        return h1, h2, g1, g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c340ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder_model, dataloader, optimizer, device):\n",
    "    best = float(\"inf\")\n",
    "    cnt_wait = 0\n",
    "    best_t = 0\n",
    "    \n",
    "    loss_func = NTXentLoss(temperature=0.10)\n",
    "    \n",
    "    encoder_model.train()\n",
    "    epoch_loss = 0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if data.x is None:\n",
    "            num_nodes = data.batch.size(0)\n",
    "            data.x = torch.ones((num_nodes, 1), dtype=torch.float32, device=data.batch.device)\n",
    "\n",
    "        h1, h2, g1, g2 = encoder_model(data)\n",
    "        \n",
    "        embeddings = torch.cat((g1, g2))\n",
    "        \n",
    "        # The same index corresponds to a positive pair\n",
    "        indices = torch.arange(0, g1.size(0), device=device)\n",
    "        labels = torch.cat((indices, indices))\n",
    "        \n",
    "        reg_loss = vicreg_loss(h1, h2, lambda_var=25, lambda_cov=25, mu=1)\n",
    "        loss = loss_func(embeddings, labels) + 0.005*reg_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(\"Epoch: {0}, Loss: {1:0.4f}\".format(epoch, epoch_loss))\n",
    "\n",
    "        if epoch_loss < best:\n",
    "            best = epoch_loss\n",
    "            best_t = epoch\n",
    "            cnt_wait = 0\n",
    "            torch.save(encoder_model.state_dict(), './pkl/best_model_'+ args.dataset_name + tag + '.pkl')\n",
    "        else:\n",
    "            cnt_wait += 1\n",
    "\n",
    "        if cnt_wait == args.patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "            \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54bcb873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder_model, dataloader, seeds, device):\n",
    "    encoder_model.eval()\n",
    "    x = []\n",
    "    y = []\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        if data.x is None:\n",
    "            num_nodes = data.batch.size(0)\n",
    "            data.x = torch.ones((num_nodes, 1), dtype=torch.float32, device=data.batch.device)\n",
    "        graph_embeds, node_embeds = encoder_model.get_embedding(data)\n",
    "        x.append(graph_embeds)\n",
    "        y.append(data.y)\n",
    "    x = torch.cat(x, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    random.shuffle(seeds)\n",
    "    seeds = seeds.tolist()\n",
    "    for _ in np.arange(10):\n",
    "        random_seed = seeds.pop()\n",
    "        split = get_split(num_samples=x.size()[0], train_ratio=0.1, test_ratio=0.8, seed=random_seed)\n",
    "        result.append(LREvaluator()(x, y, split))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e332fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/data_splits/\" + args.dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc1fdd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, test_indices, train_test_indices = [], [], []\n",
    "for i in range(10):\n",
    "    train_filename = os.path.join(path, '10fold_idx', 'train_idx-{}.txt'.format(i + 1))\n",
    "    test_filename = os.path.join(path, '10fold_idx', 'test_idx-{}.txt'.format(i + 1))\n",
    "    train_indices.append(torch.from_numpy(np.loadtxt(train_filename, dtype=int)).to(torch.long))\n",
    "    test_indices.append(torch.from_numpy(np.loadtxt(test_filename, dtype=int)).to(torch.long))\n",
    "\n",
    "if args.feature_augmentation:\n",
    "    file_name = 'train_test_splits_feat.txt'\n",
    "else:\n",
    "    file_name = 'train_test_splits_struct.txt'\n",
    "    \n",
    "train_test_filename = os.path.join(path, '10fold_idx', file_name)\n",
    "train_test_indices.append(torch.from_numpy(np.loadtxt(train_test_filename, dtype=int)).to(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "012461d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root='./data/'+args.dataset_name, name=args.dataset_name, \n",
    "                    pre_transform=T.Compose([edge_feature_transform]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84c92343",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = torch.unique(dataset.y)\n",
    "args.n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3f90ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.input_size = dataset.num_node_features\n",
    "args.pos_size = args.pos_enc_dim\n",
    "args.output_size = args.hidden_size\n",
    "args.edge_attr_size =  dataset.edge_attr.shape[1]\n",
    "args.edge_attr_v2_size =  dataset.edge_attr_v2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20971546",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug1 = A.Identity()\n",
    "\n",
    "if args.feature_augmentation:\n",
    "    # Feature Augmentation\n",
    "    aug2 = A.RandomChoice([A.FeatureDropout(pf=0.1),\n",
    "                           A.FeatureMasking(pf=0.1),\n",
    "                           A.EdgeAttrMasking(pf=0.1)], 1)\n",
    "else:\n",
    "    # Structure Augmentation\n",
    "    aug2 = A.RandomChoice([A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "                       A.NodeDropping(pn=0.1),\n",
    "                       A.EdgeRemoving(pe=0.1)], 1)\n",
    "\n",
    "model_1 = get_model(args)\n",
    "model_1.to(device)\n",
    "\n",
    "args.pos_attr = True\n",
    "args.input_size = args.pos_size\n",
    "model_2 = get_model(args)\n",
    "model_2.to(device)\n",
    "\n",
    "mlp1 = Projection(input_dim=args.hidden_size, output_dim=args.hidden_size)\n",
    "mlp2 = Projection(input_dim=args.hidden_size, output_dim=args.hidden_size)\n",
    "\n",
    "encoder_model = Encoder(model_1=model_1, model_2=model_2, mlp1=mlp1, mlp2=mlp2, aug1=aug1, aug2=aug2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcb47747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  20%|████████                                | 20/100 [00:19<01:14,  1.08it/s, loss=3.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss: 1.9233\n",
      "Epoch: 20, Loss: 3.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  40%|████████████████                        | 40/100 [00:37<00:56,  1.06it/s, loss=3.88]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Loss: 2.8883\n",
      "Epoch: 40, Loss: 3.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  60%|████████████████████████                | 60/100 [00:56<00:37,  1.07it/s, loss=2.89]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60, Loss: 1.2925\n",
      "Epoch: 60, Loss: 2.8872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):  80%|████████████████████████████████        | 80/100 [01:15<00:18,  1.07it/s, loss=2.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80, Loss: 1.3829\n",
      "Epoch: 80, Loss: 2.8393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T): 100%|███████████████████████████████████████| 100/100 [01:34<00:00,  1.06it/s, loss=1.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 1.0099\n",
      "Epoch: 100, Loss: 1.3472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(encoder_model.parameters(), lr=args.lr, weight_decay=args.l2_wd)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
    "with tqdm(total=100, desc='(T)') as pbar:\n",
    "    for epoch in range(1, 101):\n",
    "        loss = train(encoder_model, dataloader, optimizer, device)\n",
    "        pbar.set_postfix({'loss': loss})\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "088711f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(LR): 100%|█████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.657, F1Ma=0.62]\n",
      "(LR): 100%|████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.657, F1Ma=0.655]\n",
      "(LR): 100%|████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.657, F1Ma=0.657]\n",
      "(LR): 100%|█████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.657, F1Ma=0.62]\n",
      "(LR): 100%|████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.686, F1Ma=0.686]\n",
      "(LR): 100%|████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.686, F1Ma=0.644]\n",
      "(LR): 100%|████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.686, F1Ma=0.685]\n",
      "(LR): 100%|████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.657, F1Ma=0.643]\n",
      "(LR): 100%|████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.714, F1Ma=0.683]\n",
      "(LR): 100%|████████████████████████| 5000/5000 [00:01<00:00, best test F1Mi=0.657, F1Ma=0.633]\n"
     ]
    }
   ],
   "source": [
    "test_result = test(encoder_model, dataloader, train_test_indices[0], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0730f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_f1_values = [entry['micro_f1']*100 for entry in test_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ca9605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc mean = 67.1429 ± 1.9166\n"
     ]
    }
   ],
   "source": [
    "np_micro_f1_values = np.array(micro_f1_values)\n",
    "micro_f1_mean = np.mean(np_micro_f1_values)\n",
    "micro_f1_std = np.std(np_micro_f1_values)\n",
    "uncertainty = np.max(np.abs(sns.utils.ci(sns.algorithms.bootstrap(np_micro_f1_values, func=np.mean, \n",
    "                                                                      n_boot=1000), 95) - np_micro_f1_values.mean()))\n",
    "\n",
    "print(f'test acc mean = {micro_f1_mean:.4f} ± {micro_f1_std:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12d287b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
